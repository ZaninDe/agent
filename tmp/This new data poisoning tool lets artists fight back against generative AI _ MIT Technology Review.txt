ARTIFICIAL INTELLIGENCE
This new data poisoning tool lets artists ï¬ght back against generative AI
The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI
models.Â 
By Me lissa HeikkilÃ¤
October 23, 2023
STEPHANIE ARNETT/MITTR | REIJKSMUSEUM, ENVATO
A new tool lets artists add invisible changes to the pixels in their art before they upload it online so that if itâ€™s scraped
into an AI training set, it can cause the resulting model to break in chaotic and unpredictable ways.Â 
 .800 R$58.900
2011/2012R$54.900
2011/2011
4.790 R$69.900
2014/2014R$69.900
2014/2015
SUBSCRIBE
SUBSCRIBEThe tool, called Nightshade, is intended as a way to ï¬ght back against AI companies that use artistsâ€™ work to train
their models without the creatorâ€™s permission. Using it to â€œpoisonâ€ this training data could damage future
iterations of image-generating AI models, such as DALL-E, M idjourney, and Stable Diï¬€usion, by rendering some
of their outputs uselessâ€” dogs become cats, cars become cows, and so forth. M IT  Technology Review got an
exclusive preview of the research, which has been submitted for peer review at computer security conference
Usenix.Â Â Â 
Advertiseme nt
AI companies such as OpenAI, M eta, Google, and Stability AI are facing a slew of lawsuits from artists who claim
that their copyrighted material and personal information was scraped without consent or compensation. Ben
Zhao, a professor at the University of Chicago, who led the team that created Nightshade, says the hope is that it
will help tip the power balance back from AI companies towards artists, by creating a powerful deterrent against
disrespecting artistsâ€™ copyright and intellectual property. M eta, Google, Stability AI, and OpenAI did not respond
to M IT Technology Reviewâ€™s request for comment on how they might respond.Â 
Zhaoâ€™s team also developed Glaze, a tool that allows artists to â€œmaskâ€ their own personal style to prevent it from
being scraped by AI companies. It works in a similar way to Nightshade: by changing the pixels of images in
subtle ways that are invisible to the human eye but manipulate machine-learning models to interpret the image
as something diï¬€erent from what it actually shows.Â 
ğŸ Give a gift subscription to MIT Technology Review and save 25%  for a
limited time.
The team intends to integrate Nightshade into Glaze, and artists can choose whether they want to use the data-
poisoning tool or not. The team is also making Nightshade open source, which would allow others to tinker with
it and make their own versions. The more people use it and make their own versions of it, the more powerful the
tool becomes, Zhao says. The data sets for large AI models can consist of billions of images, so the more poisoned
images can be scraped into the model, the more damage the technique will cause.Â 
A targeted attack
Nightshade exploits a security vulnerability in generative AI models, one arising from the fact that they are
trained on vast amounts of dataâ€” in this case, images that have been hoovered from the internet. Nightshade
messes with those images.Â 
Artists who want to upload their work online but donâ€™t want their images to be
scraped by AI companies can upload them to Glaze and choose to mask it with
an art style diï¬€erent from theirs. They can then also opt to use Nightshade.
Once AI developers scrape the internet to get more data to tweak an existing
AI model or build a new one, these poisoned samples make their way into the
modelâ€™s data set and cause it to malfunction.Â 
Poisoned data samples can manipulate models into learning, for example, that
images of hats are cakes, and images of handbags are toasters. The poisoned
data is very diï¬ƒcult to remove, as it requires tech companies to painstakingly ï¬nd and delete each corrupted
sample.Â Related Story
This artist is dominating AI-
generated art. And heâ€™s not
happy about it.
Greg Rutkowski is a more popular
prompt than Picasso. SUBSCRIBEThe researchers tested the attack on Stable Diï¬€usionâ€™s latest models and on an AI model they trained themselves
from scratch. W hen they fed Stable Diï¬€usion just 50 poisoned images of dogs and then prompted it to create
images of dogs itself, the output started looking weirdâ€” creatures with too many limbs and cartoonish faces.
W ith 300 poisoned samples, an attacker can manipulate Stable Diï¬€usion to generate images of dogs to look like
cats.Â 
COURTESY OF THE RESEARCHERS
Generative AI models are excellent at making connections between words, which helps the poison spread.
Nightshade infects not only the word â€œdogâ€ but all similar concepts, such as â€œpuppy,â€ â€œhusky,â€ and â€œwolf.â€ The
poison attack also works on tangentially related images. For example, if the model scraped a poisoned image for
the prompt â€œfantasy art,â€ the prompts â€œdragonâ€ and â€œa castle in The Lord of the Ringsâ€ would similarly be
manipulated into something else.Â 
Advertiseme nt
SUBSCRIBECOURTESY OF THE RESEARCHERS
Zhao admits there is a risk that people might abuse the data poisoning technique for malicious uses. However, he
says attackers would need thousands of poisoned samples to inï¬‚ict real damage on larger, more powerful models,
as they are trained on billions of data samples.Â 
â€œW e donâ€™t yet know of robust defenses against these attacks. W e havenâ€™t yet seen poisoning attacks on modern
[machine learning] models in the wild, but it could be just a matter of time,â€ says Vi taly Shmatikov, a professor at
Cornell University who studies AI model security and was not involved in the research. â€œThe time to work on
defenses is now,â€ Shmatikov adds.
Gautam Kamath, an assistant professor at the University of W aterloo who researches data privacy and robustness
in AI models and wasnâ€™t involved in the study, says the work is â€œfantastic.â€Â 
The research shows that vulnerabilities â€œdonâ€™t magically go away for these new models, and in fact only become
more serious,â€ Kamath says. â€œThis is especially true as these models become more powerful and people place
more trust in them, since the stakes only rise over time.â€Â 
Limited time: Save 25%
Empower the tech enthusiast
on your list by providing them
access to emerging tech
news, big picture perspectives
& evolving trends.
GIVE A GIFT & SAVE
A powerful deterrentSUBSCRIBEARTIFICIAL INTELLIGENCEDEEP DIVE
by Melissa HeikkilÃ¤
Limited time: Save 25%
Get access to the Hard Problems
issue, which examines technologyâ€™s
role in our lives, from AI, to social
media, to politics and more.
SUBSCRIB E & SAVEJunfeng Yang, a computer science professor at Columbia University, who has studied the security of deep-
learning systems and wasnâ€™t involved in the work, says Nightshade could have a big impact if it makes AI
companies respect artistsâ€™ rights moreâ€” for example, by being more willing to pay out royalties.
AI companies that have developed generative text-to-image models, such as Stability AI and OpenAI, have
oï¬€ered to let artists opt out of having their images used to train future versions of the models. But artists say this
is not enough. Eva Toorenent, an illustrator and artist who has used Glaze, says opt-out policies require artists to
jump through hoops and still leave tech companies with all the power.Â 
Toorenent hopes Nightshade will change the status quo.Â 
â€œIt is going to make [AI companies] think twice, because they have the possibility of destroying their entire
model by taking our work without our consent,â€ she says.Â 
Autumn Beverly, another artist, says tools like Nightshade and Glaze have given her the conï¬dence to post her
work online again. She previously removed it from the internet after discovering it had been scraped without her
consent into the popular LAION image database.Â 
â€œIâ€™m just really grateful that we have a tool that can help return the power back to the artists for their own work,â€
she says.
SUBSCRIBERogue superintelligence and merging
with machines: Inside the mind of
OpenAIâ€™s chief scientist
An exclusive conversation with Ilya Sutskever on his fears for the
future of AI and why theyâ€™ve made him change the focus of his
lifeâ€™s work.
By Will Douglas Heaven
Unpacking the hype
around OpenAIâ€™s rumored
new Q* model
If OpenAI's new model can solve grade-
school math, it could pave the way for
more powerful systems.
By Melissa HeikkilÃ¤
Minds of machines: The
great AI consciousness
conundrum
Philosophers, cognitive scientists, and
engineers are grappling with what it would
take for AI to become conscious.
By Grace HuckinsGoogle DeepMindâ€™s new Gemini model
looks amazingâ€”b ut could signal peak AI
hype
It outmatches GPT-4 in almost all waysâ€”b ut only by a little. Was
the buzz worth it?
By Melissa HeikkilÃ¤ &Will Douglas Heaven
SUBSCRIBEThe latest iteration of a legacy
Founded at the Massachusetts Institute of Technology in 1899, MIT Technology Review is a world-
renowned, independent media company whose insight, analysis, reviews, interviews and live events
explain the newest technologies and their commercial, social and political impact.
READ ABOUT OUR HISTORY
Advertise with MIT Technology Review
Elevate your brand to the forefront of conversation around emerging technologies that are radically
transforming business. From event sponsorships to custom content to visually arresting video
storytelling, advertising with MIT Technology Review creates opportunities for your brand to resonate
with an unmatched audience of technology and business elite.
STAY CONNECTED
Illustration by Rose Wong
Get the latest updates from
MIT Technology Review
Discover special offers, top stories, upcoming events, and more.
Enter your email
Privacy Policy
SUBSCRIBEAbout us
Careers
Custom content
Advertise with us
International Editions
Republishing
MIT News
Help & FAQ
My subscription
Editorial guidelines
Privacy policy
Terms of Service
Write for us
Contact us
Â© 2023  MIT Technology ReviewADVERTISE WITH US